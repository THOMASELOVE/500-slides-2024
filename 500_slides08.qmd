---
title: "500 Class 08"
author: "<https://thomaselove.github.io/500-2024/>"
date: "2024-03-07"
date-format: iso
format: 
  beamer:
    theme: Madrid
    colortheme: lily
    fig-align: center
---

## Today's Agenda

- Some Extensions to Matching (finally)
- Tanenbaum 2019
- Elbadawi 2021
- Discussion of Rosenbaum Chapter 7

# Some Extensions to Propensity Matching

## Is Regression Adjustment Unnecessary?

- Matching and stratification are old and trusted methods of adjustment for observational studies, but the difficulty of implementing them led earlier practitioners to prefer regression.
- Modern extensions to matching methods let us perform optimal matches, full matches and optimal full matches, and to control imbalance (or at least reduce bias reduction) in ways that have become attainable only in recent years.

Good references include Rosenbaum (2010) and Hansen (2004) for example.

## General Approaches to Optimal or Near-Optimal Constrained Matching

1. Calculate propensity scores
2. Establish a **distance matrix**

This is just a table with one row for each treated subject and one column for each potential control.

- The "distances" can be squared differences in propensity scores between the subjects, Mahalanobis distances, or something else.
- To use calipers, we set to $\infty$ all cells in the table corresponding to a propensity difference which exceeds the caliper.

## A Small Distance Matrix

Consider four treated subjects (T1, T2, T3 and T4) and six control subjects (C1, C2, C3, C4, C5 and C6.) 

- We have a difference score (perhaps the absolute difference in propensity for treatment) for each comparison. Some of these are infinite.
- We also have each subject categorized as (Y)oung or (O)ld, and we haven't decided yet how important this is for our matching.

Subject | C1 (Y) | C2 (O) | C3 (O) | C4 (Y) | C5 (O) | C6 (O)
:-----: | -----: | -----: | -----: | -----: | -----: | -----: 
T1 (Y)  | .23 | .47 | .39 | $\infty$ | .51 | .35
T2 (O)  | .45 | $\infty$ | .28 | .31 | .42 | $\infty$
T3 (O)  | $\infty$ | .35 | $\infty$ | .27 | .44 | .28
T4 (O)  | .31 | .26 | .51 | .29 | $\infty$ | .24

## OK, so Who Gets Matched?

Subject | C1 (Y) | C2 (O) | C3 (O) | C4 (Y) | C5 (O) | C6 (O)
:-----: | -----: | -----: | -----: | -----: | -----: | -----: 
T1 (Y)  | .23 | .47 | .39 | $\infty$ | .51 | .35
T2 (O)  | .45 | $\infty$ | .28 | .31 | .42 | $\infty$
T3 (O)  | $\infty$ | .35 | $\infty$ | .27 | .44 | .28
T4 (O)  | .31 | .26 | .51 | .29 | $\infty$ | .24

>- Now, who gets matched?
>- Treated subject T1 matches to C1
>- T2 matches to C3
>- T3 matches to C4 (or maybe C6 - is age important?)
>- T4 matches to C6 (or C2, or C4, hmmm....)

## Almost Exact Matching

- Suppose a few of the covariates are of enormous importance - want to match exactly on them wherever possible.

We could add a penalty (but perhaps not an infinite penalty) to the distance matrix when the specified covariates fail to match, and that is the main approach that we use.

- Adding 2 to the Mahalanobis distance for mismatches roughly doubles the importance of that covariate as compared to the others, for example.

There's a lot of active work in this area developing various algorithms that permit finer control.

## "Fine Balance" in Matching

- Constrain optimal matching that forces a nominal variable to be balanced, without restricting who is matched to whom.

This is especially useful if...

- you have a nominal variable with many levels
- you have a rare binary variable that is difficult to control using a distance
- you are focused on the interaction of several nominal variables

It is also possible to get specific imbalance patterns.

## Fine Balance: Initial Distance Matrix

Subject | C1 (Y) | C2 (O) | C3 (O) | C4 (Y) | C5 (O) | C6 (O)
:-----: | -----: | -----: | -----: | -----: | -----: | -----: 
T1 (Y)  | .23 | .47 | .39 | $\infty$ | .51 | .35
T2 (O)  | .45 | $\infty$ | .28 | .31 | .42 | $\infty$
T3 (O)  | $\infty$ | .35 | $\infty$ | .27 | .44 | .28
T4 (O)  | .31 | .26 | .51 | .29 | $\infty$ | .24

Suppose we want to get optimal balance on the propensity score while matching perfectly on the age category (Y/O).

- We have 4 treated subjects (1 young, 3 old)
- We have 6 potential controls (2 young, 4 old)
- So we need to remove 1 young and 1 old in matching

## Fine Balance: Augmented Distance Matrix

Subject | C1 (Y) | C2 (O) | C3 (O) | C4 (Y) | C5 (O) | C6 (O)
:-----: | -----: | -----: | -----: | -----: | -----: | -----: 
T1 (Y)  | .23 | .47 | .39 | $\infty$ | .51 | .35
T2 (O)  | .45 | $\infty$ | .28 | .31 | .42 | $\infty$
T3 (O)  | $\infty$ | .35 | $\infty$ | .27 | .44 | .28
T4 (O)  | .31 | .26 | .51 | .29 | $\infty$ | .24
*Extra 1* | 0 | $\infty$ | $\infty$ | 0 | $\infty$ | $\infty$ 
*Extra 2* | $\infty$ | 0 | 0 | $\infty$ | 0 | 0

Add 2 rows to the matrix, then run the match

- *Extra 1* pulls away one young control
- *Extra 2* pulls away one old control

The binary age category will be perfectly balanced across the matched sample, but the partners within each individual pair are not required to be in the same age category.

## Fine Balance General Procedure

To get the minimum distance match with fine balance (on a nominal covariate, say GROUP)...

1. Cross tabulate GROUP with treatment indicator
2. Determine # of controls to remove from each category of GROUP to achieve perfect balance
3. Add one row for each control that must be removed, with 0 distance to its own category and infinite distance to all others
4. Find an optimal match for this square matrix
5. Discard extra rows and their matched controls

# Full Matching

## Full Matching in Observational Studies

- In the past, it has been tough to implement full matching in observational studies, even though it is appealing in principle.
- Alignment of comparable treated and control subjects is as good as any alternate method, and potentially much better.
- Hansen (2004) modifies full matching with modifications to minimize variance as well as bias

In this example,

- Optimal full matching removes as much as 99% of the bias along a PS on which treated and control means are separated by 1.1 SD's. 
- Reduces to insignificance biases along 27 covariates, while making use of more, not less, of the data than regression based analyses.

## Hansen (2004) SAT Coaching Study

- Survey of a random sample of 1995-1996 SAT test takers about their preparation
- 12% of respondents had completed extracurricular test preparation courses
- Matching looked unattractive to the original researchers due to significant reduction in sample size, but they only considered 1:1 matching.
- Do 1:k matching options look better?

---

![](c08/figures/hansen_1.png){width=90%}

---

![](c08/figures/hansen_2.png){width=90%}

---

![](c08/figures/hansen_3.png){width=90%}

---

![](c08/figures/hansen_4.png){width=90%}

---

![](c08/figures/hansen_5.png){width=90%}

---

![](c08/figures/hansen_6.png){width=90%}

---

![](c08/figures/hansen_7.png){width=90%}

---

![](c08/figures/hansen_8.png){width=90%}

## SAT Coaching Study Results

- Raw differences of treated and control group means were 41 points on Math and 9 on Verbal
- Full matching leads to aggregate contrasts of 26 points on Math and 1 point on the verbal.
    - Standard errors for these estimates are around 5 points.
- Surprised that Verbal effect is so small?
    - Control is not "no prep at all"
    - Estimated effect of treatment on the controls is 3 for Math and -8 on Verbal.
- Method doesn't require homogeneity of coaching effects.
- Whether and to what degree coaching is beneficial appears to vary greatly across students.


# Tanenbaum 2017

## Details to come.

---

![](c08/figures/jt01.png)

--- 

![](c08/figures/jt02.png)

# Elbadawi 2021

## Details to come.

# Discussion of Rosenbaum, Chapter 7

## Rosenbaum, Chapter 7

1. What was the most important thing you learned from reading Chapter 7?
2. What was the muddiest, least clear thing that arose in your reading?
